{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Getting Started.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZB16RvwSQYD"
      },
      "source": [
        "Copyright &copy; CAMMA, ICube, University of Strasbourg. All Rights Reserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8OJJD-kSTcD"
      },
      "source": [
        "<div>\n",
        "<a href=\"https://cholectriplet2021.grand-challenge.org/\">\n",
        "<img src=\"https://raw.githubusercontent.com/DpkApt/evis_at/master/pictures/header.png\" align=\"left\"/>\n",
        "</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu8aMfE5S74v"
      },
      "source": [
        "## <h1><center>Getting Started</center></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceAz2udUTolO"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hLKusP2TpGJ"
      },
      "source": [
        "In this notebook, we provide sample code to help familiarize yourself with the challenge, the dataset and the metrics. These are minimal examples to help illustrate a simple deep learning pipeline applied on a small subset of the Action Triplet dataset, **CholecT50**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQdB8OZWjcb"
      },
      "source": [
        "# Data Loading and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyjsFvOg3gso"
      },
      "source": [
        "# Import necessary libraries for this module\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "print(\"Libraries successfully imported!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1K8d3LssreJ"
      },
      "source": [
        "Here, we use a small subset of the CholecT50 dataset available at this link: https://seafile.unistra.fr/f/ba1427a82ecc4ce18566/?dl=1. If you are running this notebook on Colab, you can run the cell below to download and unzip the dataset to the current directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSeXm7OSm_D3"
      },
      "source": [
        "# Ignore this cell if you have already downloaded and extracted the dataset\n",
        "\n",
        "!wget -O CholecT50_sample.zip https://seafile.unistra.fr/f/ba1427a82ecc4ce18566/?dl=1\n",
        "!unzip CholecT50_sample.zip\n",
        "\n",
        "print(\"Dataset successfully extracted!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvzcRoaNYds7"
      },
      "source": [
        "# Change the dataset_path mentioned below appropriately if you have exracted the data to a different directory \n",
        "\n",
        "dataset_path = './CholecT50_sample/'\n",
        "\n",
        "data_path = os.path.join(dataset_path, 'data')\n",
        "triplet_path = os.path.join(dataset_path, 'triplet')\n",
        "dict_path = os.path.join(dataset_path, 'dict')\n",
        "video_names = os.listdir(data_path)                                   \n",
        "\n",
        "print(\"Dataset paths successfully defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk2y7u2s7i5J"
      },
      "source": [
        "# Create dictionary mapping triplet ids to readable label\n",
        "\n",
        "with open(os.path.join(dict_path, 'triplet.txt'), 'r') as f:\n",
        "  triplet_info = f.readlines()\n",
        "  triplet_dict = {}\n",
        "  for l in triplet_info:\n",
        "    triplet_id, triplet_label = l.split(':')\n",
        "    triplet_dict[int(triplet_id)] = triplet_label.rstrip()\n",
        "\n",
        "print('Random triplet id and its human readable label\\n')\n",
        "random_triplet_id = np.random.choice(list(triplet_dict.keys()))\n",
        "print('Triplet id: ', random_triplet_id, '\\nReadable label: ', triplet_dict[random_triplet_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLDza6yst9-I"
      },
      "source": [
        "def generator(data_path, triplet_path, video_names, batch_size, shuffle_videos=False):\n",
        "  \"\"\" Defines a simple generator that returns sequential batches of input images and  their \n",
        "      corresponding triplet labels, video names, and frame ids.\n",
        "        Args:\n",
        "            data_path:     Path to directory containing a folder for each video\n",
        "            triplet_path:  Path to folder containing a txt file for each video\n",
        "                           listing the frame id, and binary label for all of the 100 considered \n",
        "                           triplet classes\n",
        "            video_names:   Names of the videos that will be retruned by this generator. These names\n",
        "                           should correpond to a folder in data_path and a txt file in triplet path\n",
        "            batch_size:    Batch size of outputs yielded by the generator\n",
        "            shuffle_videos:To perform a shuffling of videos (Note: frames will be returned sequentially!)  \n",
        "        Returns:\n",
        "            image batch     : Batch of images\n",
        "            triplet batch   : Batch of triplet labels ([N] vectors)\n",
        "            video_name_batch: Batch of video name strings\n",
        "            frame_id_batch  : Batch of integer frame ids\n",
        "    \"\"\"\n",
        "\n",
        "  if shuffle_videos:\n",
        "    video_names = np.random.shuffle(video_names)\n",
        "\n",
        "  image_batch, triplet_batch, video_name_batch, frame_id_batch = [], [], [], []\n",
        "\n",
        "  for video_name in video_names:\n",
        "    with open(os.path.join(triplet_path, video_name + '.txt'), mode='r') as infile:\n",
        "        reader = csv.reader(infile)\n",
        "\n",
        "        for line in reader:\n",
        "          line = np.array(line, np.uint8)\n",
        "          frame_id, triplet_label = line[0], line[1:]\n",
        "          image_path = os.path.join(data_path, video_name, \"%06d.png\" %frame_id)\n",
        "          image = np.array(Image.open(image_path), np.float32) / 255.0\n",
        "\n",
        "          image_batch.append(image)\n",
        "          triplet_batch.append(triplet_label)\n",
        "          video_name_batch.append(video_name)\n",
        "          frame_id_batch.append(int(frame_id))\n",
        "\n",
        "          if len(frame_id_batch) == batch_size:\n",
        "            yield image_batch, triplet_batch, video_name_batch, frame_id_batch\n",
        "            image_batch, triplet_batch, video_name_batch, frame_id_batch = [], [], [], []\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0SZBNWSyf7g"
      },
      "source": [
        "batch_size = 8\n",
        "gen = generator(data_path, triplet_path, video_names, batch_size)\n",
        "\n",
        "for images, triplet_labels, video_names, frame_ids in gen:\n",
        "  for batch in range(batch_size):\n",
        "    print('\\nVisualizing image...\\n')\n",
        "    print('Video name: ', video_names[batch], ' Frame_id', frame_ids[batch])\n",
        "    plt.imshow(images[batch])\n",
        "    plt.show()\n",
        "    print('\\nEncoding showing which of the 100 considered action triplets are represented in the image\\n')\n",
        "    print(triplet_labels[batch])\n",
        "    print('\\nReadable labels\\n')\n",
        "    for triplet in np.where(triplet_labels[batch])[0]:\n",
        "      print(triplet_dict[triplet])\n",
        "    \n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBl9-4gOWof0"
      },
      "source": [
        "#  Building and Running Models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0rkzEkh4ysO"
      },
      "source": [
        "We build and perform a simple forward pass of an image through a few layer convolutional network to predict the probability of each of the considered triplets being represented in the input image.\n",
        "\n",
        "Note: Please run the cells in the previous module Data Loading and Visualization before running this module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxDnLbKQ35t7"
      },
      "source": [
        "# Import necessary libraries for this module. You can skip ahead if you prefer a PyTorch based example.\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"Libraries successfully imported!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JbM-RYZEJf4"
      },
      "source": [
        "Defining a simple neural network using tf.keras. You can skip ahead if you prefer to use torch.nn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl65rM9IAmqi"
      },
      "source": [
        "# Defining the neural network architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(\n",
        "    filters=16, kernel_size=3, strides=2, activation=\"relu\", input_shape=(480, 854, 3))\n",
        ")                 \n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")) \n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")) \n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")) \n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")) \n",
        "model.add(tf.keras.layers.Flatten())                                     \n",
        "model.add(tf.keras.layers.Dense(units=4096, activation=\"relu\"))                 \n",
        "model.add(tf.keras.layers.Dense(units=2048, activation=\"relu\"))            \n",
        "model.add(tf.keras.layers.Dense(units=100, activation=\"sigmoid\"))    \n",
        "\n",
        "print(\"Neural network architecture successfully defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ_Yp4e6A_Q-"
      },
      "source": [
        "model.build([1, 480, 854, 3])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfaJhd471PZJ"
      },
      "source": [
        "Performing a simple forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aZ4baDLDAYS"
      },
      "source": [
        "input_4d = np.expand_dims(images[0], axis=0)\n",
        "print('Performing a simple forward pass on our untrained network for a test image')\n",
        "plt.imshow(images[0])\n",
        "plt.show()\n",
        "print('\\nPrediction\\n')\n",
        "print(model.predict(input_4d)[0])\n",
        "print('\\nLabel\\n')\n",
        "print(triplet_labels[0])\n",
        "print('\\nReadable label\\n')\n",
        "for triplet_id in np.where(triplet_labels[0])[0]:\n",
        "  print(triplet_dict[triplet_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2M05KcCQ0eA"
      },
      "source": [
        "[**OPTIONAL**] Using PyTorch to make a prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvjIpgGkRKge"
      },
      "source": [
        "# Import necessary libraries for this module.\n",
        "\n",
        "import torch\n",
        "import numpy as np  \n",
        "from torch import nn\n",
        "from torch.nn import Module  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj0iOFBzRnSp"
      },
      "source": [
        "class MyModel(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, h, w):\n",
        "        super(MyModel, self).__init__()\n",
        "        # input to first hidden layer\n",
        "        self.conv1 = nn.Conv2d(3, 32, (3,3))\n",
        "        self.conv2 = nn.Conv2d(32, 64, (3,3))\n",
        "        self.conv3 = nn.Conv2d(64, 128, (3,3))\n",
        "        self.conv4 = nn.Conv2d(128, 256, (3,3))\n",
        "        self.pool1 = nn.MaxPool2d((3,3), stride=(2,2))\n",
        "        self.pool2 = nn.MaxPool2d((2,2), stride=(2,2))\n",
        "        self.pool3 = nn.MaxPool2d((2,2), stride=(2,2))\n",
        "        self.h   = int(h/8 - 4)\n",
        "        self.w   = int(w/8 - 4)\n",
        "        self.mlp = nn.Linear(self.h*self.w*256, 100)        \n",
        "        self.act1 = nn.ReLU()\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.act3 = nn.ReLU()        \n",
        "        self.act4 = nn.Sigmoid()\n",
        "    # forward propagate input\n",
        "    def forward(self, X):\n",
        "        # input to first hidden layer\n",
        "        X = self.conv1(X)\n",
        "        X = self.act1(X)\n",
        "        X = self.pool1(X)\n",
        "        # second hidden layer\n",
        "        X = self.conv2(X)\n",
        "        X = self.act2(X)\n",
        "        X = self.pool2(X)\n",
        "        # second hidden layer\n",
        "        X = self.conv3(X)\n",
        "        X = self.act3(X)\n",
        "        X = self.pool3(X)\n",
        "        # second hidden layer\n",
        "        X = self.conv4(X)\n",
        "        # flatten\n",
        "        X = X.view(-1, self.h*self.w*256)\n",
        "        # output layer\n",
        "        X = self.mlp(X)\n",
        "        X = self.act4(X)\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4z0Z9rcSMMd"
      },
      "source": [
        "input_4d = np.expand_dims(images[0], axis=0)\n",
        "# Converting to Channel first. NHWC --> NCHW\n",
        "input_4d = np.transpose(input_4d, [0, 3, 1, 2])\n",
        "input_4d = torch.from_numpy(input_4d)\n",
        "print('Performing a simple forward pass on our untrained network for a test image')\n",
        "plt.imshow(images[0])\n",
        "plt.show()\n",
        "print('\\nPrediction\\n')\n",
        "model  = MyModel(480, 854)\n",
        "print(model(input_4d)[0])\n",
        "print('\\nLabel\\n')\n",
        "print(triplet_labels[0])\n",
        "print('\\nReadable label\\n')\n",
        "for triplet_id in np.where(triplet_labels[0])[0]:\n",
        "  print(triplet_dict[triplet_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_4F2W7uWs5V"
      },
      "source": [
        "#  Metrics and Evaluation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLK2JMF4ET2K"
      },
      "source": [
        "# Import necessary libraries for this module\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s3vCa2_zwWd"
      },
      "source": [
        "def _compute_AP(gt_labels, pd_probs, valid=None):\n",
        "    \"\"\" Compute the average precision (AP) of each of the 100 considered triplets.\n",
        "        Args:\n",
        "            gt_labels: 1D (batch of) vector[N] of integer values 0's or 1's for the groundtruth labels.\n",
        "            pd_probs:  1D (batch of) vector[N] of float values [0,1] for the predicted labels.\n",
        "        Returns:\n",
        "            results:   1D vector[N] of AP for each class \n",
        "    \"\"\"\n",
        "    gt_instances  = np.sum(gt_labels, axis=0)\n",
        "    pd_instances  = np.sum(pd_probs, axis=0)\n",
        "    computed_ap   = average_precision_score(gt_labels, pd_probs, average=None)\n",
        "    actual_ap     = []\n",
        "    num_classes   = np.shape(gt_labels)[-1]\n",
        "    for k in range(num_classes):\n",
        "        if ((gt_instances[k] != 0) or (pd_instances[k] != 0)) and not np.isnan(computed_ap[k]):\n",
        "            actual_ap.append(computed_ap[k])\n",
        "        else:\n",
        "            actual_ap.append(\"n/a\")\n",
        "    return actual_ap\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyNsVlS7zz_i"
      },
      "source": [
        "def _average_by_videos(results):\n",
        "    \"\"\" Compute the average AP of each triplet class across all the videos\n",
        "        and mean AP of the model on the triplet predictions.\n",
        "        Args:\n",
        "            results:   1D (batch of) vector of AP for each class. One member of the batch corresponds\n",
        "                       to one video\n",
        "        Returns:\n",
        "            AP:   1D vector[N] of AP for each class averaged by videos\n",
        "    \"\"\"\n",
        "    n = results.shape[-1]\n",
        "    AP = []\n",
        "    for j in range(n):\n",
        "        x = results[:,j]\n",
        "        x = np.mean([float(a) for a in x if (str(a)!='n/a') ])\n",
        "        if np.isnan(x):\n",
        "            AP.append(\"n/a\")\n",
        "        else:          \n",
        "            AP.append(x)\n",
        "    mAP = np.mean( [i for i in AP if i !='n/a'])\n",
        "    return np.array(AP), mAP\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_RjhHJ6z1x6"
      },
      "source": [
        "# example usage. Here we use randomly generated ground truth and predicted values\n",
        "\n",
        "half_0s = np.zeros(shape=[20,100], dtype=np.int64)\n",
        "half_1s = np.ones(shape=[20,100], dtype=np.int64)\n",
        "\n",
        "vid230_gt = np.concatenate((half_0s, half_1s), axis=0)\n",
        "vid231_gt = np.concatenate((half_1s, half_0s), axis=0)\n",
        "\n",
        "vid230_pd = np.random.random((40,100))\n",
        "vid231_pd = np.random.random((40,100))\n",
        "\n",
        "non_null  = list(range(0,93)) # for ignoring the null triplets\n",
        "ap_vid230 = _compute_AP(gt_labels=vid230_gt, pd_probs=vid230_pd, valid=non_null)\n",
        "ap_vid231 = _compute_AP(gt_labels=vid231_gt, pd_probs=vid231_pd, valid=non_null)\n",
        "\n",
        "ap_vid  = np.stack([ap_vid230, ap_vid231,], axis=0)\n",
        "AP,mAP  = _average_by_videos(results=ap_vid)\n",
        "\n",
        "print(AP, \"\\nmAP = \",mAP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3LZGSrs85K6"
      },
      "source": [
        "#  Saving Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxIoiliuBUXZ"
      },
      "source": [
        "# Import necessary libraries for this module\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import csv\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l8Cnrq6AepS"
      },
      "source": [
        "# Ignore this cell if you have already downloaded and extracted the dataset or run the Data Loading and Visualization section.\n",
        "\n",
        "!wget -O CholecT50_sample.zip https://seafile.unistra.fr/f/ba1427a82ecc4ce18566/?dl=1\n",
        "!unzip CholecT50_sample.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e5-i9jt88WW"
      },
      "source": [
        "For the final challenge evaluation, each participant will need to create a .txt output file for each test video and save as {Video-name}_{team-name}_results.txt. This txt file should contain the predicted probabilities for each action triplet. Here, we provide a standalone example to execute this workflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxVGlVamGB5C"
      },
      "source": [
        "# Defining a simple generator to return images sequentially \n",
        "\n",
        "def eval_generator(data_path):\n",
        "  \"\"\" Defines a simple generator that returns sequential batches of input images and  their \n",
        "      corresponding frame ids. \n",
        "        Args:\n",
        "            data_path:     Path to directory containing a folder for each video\n",
        "        Returns:\n",
        "            image batch     : Batch of images\n",
        "            frame_id_batch  : Batch of integer frame ids\n",
        "    \"\"\"\n",
        "  images_and_frame_ids = [(x, int(x.split('.')[0])) for x in os.listdir(data_path)]\n",
        "  # Sorting inputs by frame id\n",
        "  images_and_frame_ids = sorted(images_and_frame_ids, key=lambda x: x[1])\n",
        "  for image_name, frame_id in images_and_frame_ids:\n",
        "    image_path = os.path.join(data_path, image_name)\n",
        "    image = np.array(Image.open(image_path), np.float32) / 255.0\n",
        "    yield np.expand_dims(image, 0), frame_id\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwtK1LqzGDe6"
      },
      "source": [
        "# Defining a model that takes an input 4d input of size (Batch x Height x Width x Channels)\n",
        "# and returns a prediction. Here, we use a random model.\n",
        "\n",
        "def my_model(input):\n",
        "  batch, _, _, _ = input.shape\n",
        "  return np.random.uniform(0, 1, (batch, 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubAtsVDQ_7jg"
      },
      "source": [
        "Save results to txt. This output can be visualized by clicking on the files icon on the vertical tab on the left of your colab notebook or by navigating to the appropriate directory on your local machine if you are running this notebook on jupyter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiBxjweq87Ui"
      },
      "source": [
        "input_dir = './CholecT50_sample/data/VID230'\n",
        "output_dir = './'\n",
        "video_name = os.path.basename(input_dir)\n",
        "output_filename = video_name + '_' + 'MyTeamName_results.txt'\n",
        "output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "video_results = []\n",
        "for image, frame_id in eval_generator(input_dir):\n",
        "  result = [frame_id]\n",
        "  predictions = my_model(image)\n",
        "  result.extend(list(predictions[0]))\n",
        "  video_results.append(result)\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(video_results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}